%\VignetteIndexEntry{nplr}

% Packages
\documentclass{article}
\usepackage[american]{babel}
\usepackage{floatrow}
\usepackage{url}
%\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

% Bibliography
\usepackage[backend=bibtex, sorting=none]{biblatex}
\bibliography{references}
\usepackage{csquotes}

\begin{filecontents*}{references.bib}
@article{Richards,
    title = {A flexible growth function for empirical use.},
    author = {FJ, Richards},
    year = {1959},
    journal={J Exp Bot.},
    volume={10},
    pages={290--300}
}
@article{Giraldo,
    author={J, Giraldo and NM, Vivas and E, Vila and A, Badia},
    title={Assessing the (a)symmetry of concentration-effect curves: empirical versus mechanistic models},
    year={2002},
    journal={Pharmacol Ther.},
    volume={95},
    number={1},
    pages={21--45}
}
@article{Motulsky,
    author={HJ, Motulsky and RE, Brown},
    title={Assessing the (a)symmetry of concentration-effect curves: empirical versus mechanistic models},
    year={2006},
    journal={BMC Bioinformatics},
    volume={9},
    pages={7--123}
}
@misc{NCI,
  url = {https://wiki.nci.nih.gov/display/NCIDTPdata/NCI-60+Growth+Inhibition+Data}
}

\end{filecontents*}

% Page settings
\setlength{\textwidth}{15cm} 
\setlength{\textheight}{24cm}
\setlength{\oddsidemargin}{0cm} 
\setlength{\evensidemargin}{2cm}
\setlength{\topmargin}{-2cm}

\begin{document}

\title{R package \emph{nplr} \\
  \emph{n-parameter logistic regressions}}
\author{Frederic Commo \& Briant M. Bot}
\maketitle\thispagestyle{empty}

% Chunk setup
<<setup, include=FALSE>>=
require(knitr)
opts_chunk$set(fig.align='center', fig.width=5.5, fig.height=5, dev='pdf', prompt=TRUE, comment=NA, highlight=FALSE, tidy=FALSE)
@

\section{Introduction}
\subsection{Overview}
In in-vitro experiments, the aim of drug response analyses is usually to estimate the drug concentration required to reach a given cell line growth inhibition rate - typically the 50\% inhibitory concentration ($IC_50$), which inhibits 50\% of the proliferation, compared with an untreated control. This estimation can be achieved by modeling the inhibition rate observed under a range of drug concentrations.  
Once the model is fitted, the x values (drug concentrations) can be estimated from the y values (inhibition rates) by simply inverting the function.\\
The most commonly used model for drug response analysis is the Richards' equation \cite{Richards}, also refered to as a 5-parameter logistic regression \cite{Giraldo}:

$$y = B + \dfrac{T - B}{\left [1 + 10^{b(x_{mid} - x)} \right ]^s}$$
\\
Where $B$ and $T$ are the bottom and top asymptotes,  and $b$, $x_{mid}$ and $s$ are the Hill slope, the x-coordinate at the inflexion point and an asymetric coefficient, respectively.\\
The \texttt{nplr} package we have developed is based on the full 5-parameter model, and provides several options in order 
to compute flexible weighted n-parameter logistic regression: n can be explicitly specified, from 2 to 5, or \texttt{nplr} can compare all of these models, and return the optimal one (by default, \texttt{npars="all"}), with respect to a weighted Goodness-of-Fit estimator. See the \texttt{nplr} documentation for more details.
\\
During the fitting step, all of the parameters are optimized, simultaneously, using a Newton method (\texttt{nlm}, R package \texttt{stats}). The objective function to minimize is a weighted sum of squared errors:

$$ sse(Y) = \Sigma_i{w_i.(\hat{y}_i - y_i)^2}, i=1,...,n $$

The weights, $wi$, used in the objective function can be computed using 3 possible methods, as follows:

\begin{itemize}
  \renewcommand\labelitemi{--}
  \item{residuals weights:} $w_i = \left (\frac{1}{res_i}\right )^p, i=1,...,n\ values$
  \item{standard weights:} $w_{ir} = \frac{1}{Var(y_r)}, r=1,...,r\ replicated\ conditions$ 
  \item{general weights:} $w_{i} = \left (\frac{1}{\hat{y}_i}\right )^p, i=1,...,n\ values$
\end{itemize}

where $p$ is a tuning parameter. The \texttt{standard weights} and the \texttt{general weights} are described in \cite{Motulsky}.

\subsection{Functions in \texttt{nplr}}
The main function is simply \texttt{nplr}, and requires 2 main arguments: a vector of \texttt{x} and a vector of \texttt{y}.\\
The $npars$ argument allows a user to run specific n-parameter models, n from 2 to 5, while the default value, \texttt{npars="all"}, asks the function to test which model fits the best the data, according to a weighted Goodness-of-Fit estimator.\\
In some situations, the x values may need to be log-transformed, e.g. x is provided as original drug concentrations. In such case, setting $useLog=TRUE$ in \texttt{nplr()} will apply a $Log_10$ transformation on the x values.
\\
The \texttt{nplr()} function has been optimized for fitting curves on y-values passed as proportions of control, between 0 to 1.
If data are supplied as original response values, e.g. optic density measurements, the \texttt{convertToProp()} function may be helpful. In drug-response curve fitting, a good practice consists in adjusting the signals on a $T_0$ and a $control$  (Ctrl) values. Providing this values, the proportion values, $y_p$, are computed as:

$$y_p = \frac{y - T_0}{Ctrl - T_0}$$
\\
where $y$, $T_0$ and $Ctrl$ are the observed values, the 'time zero' and the 'untreated control', respectively.\\
Note that if neither $T_0$ nor $Ctrl$ are provided, the default behavior of \texttt{convertToProp()} is to adjust the values as proportions of the $min$ and $max$ of $y$. In that case, the user should be aware that $y = 0.5$ may not correspond to a $IC50$, but rather to a $EC50$ (the half-effect between the maximum and the minimum of the observed effects).
\\

In a drug-response (or progression) curve fitting context, typical needs are to invert the function in order to estimate the x value, e.g. the $IC_50$, given a $y$ value, e.g. the 0.5 survival rate. To do so, the implemented \texttt{getEstimates()} method takes 2 arguments: the model (an instance of the class nplr), and one (or a vector of) target(s). \texttt{getEstimates()} returns the corresponding x values and their estimated confidence intervals, as specified by \texttt{conf.level}.


\section{Examples}
The examples below use some samples of the NCI-60 Growth Inhibition Data. The full data can be downloaded at \cite{NCI}.\\
For the purpose of the demonstration, the supplied drug concentrations have been re-exponentiated.

\subsection{Example 1}
\subsubsection{Fitting a model}
<<message=FALSE, warning=FALSE>>=
require(nplr)
@

The first example fits a simple drug-response curve: the PC-3 cell line treated with Thioguanine, 19 points without replicates.
<<>>=
path <- system.file("extdata", "pc3.txt", package="nplr")
pc3 <- read.table(path, header=TRUE)
np1 <- nplr(x=pc3$CONC, y=pc3$GIPROP)
@

Calling the object returns the fitting summary for the model.
<<>>=
np1
@


\subsubsection{Visualizing the model}
A specific \texttt{plot()} function has been implemented in order to visualize the results.
<<simpleExample, include=FALSE>>=
plot(np1, main="PC-3 cell line. Response to Thioguanine", cex.main=1.2)
@

<<fig1, ref.label='simpleExample'>>=
    @

\vspace{.5in}
This function has several predefined graphical parameters, and some of them can be overwritten.  
However, a convenient way to draw simplest or customized plots is shown in the example below:
<<custom, include=FALSE>>=
op <- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(np1, pcol="grey40", lcol="skyblue1", showTarget=.5, showInfl=TRUE, main="Using plot()", cex.main=1.5)
x1 <- getX(np1); y1 <- getY(np1)
x2 <- getXcurve(np1); y2 <- getYcurve(np1)
plot(x1, y1, pch=15, cex=2, col="tan1", xlab=expression(Log[10](conc)), ylab="Prop", main="My plot", cex.main=1.5)
lines(x2, y2, lwd=5, col="seagreen4")
par(op)
@

<<fig2, ref.label='custom', fig.width=10, echo=3:9>>=
    @

\subsubsection{Accessing the performances}
Once the model is built, several accessor functions allow to get access to the performances of the model, and its parameters.
<<>>=
getGoodness(np1)
getStdErr(np1)
getPar(np1)
@

\subsubsection{Estimating the drug concentrations}
The purpose of the fitting is to estimate the response to the drug. To do so, \texttt{nplr} provides 2 estimates: the area under the curve (AUC), and the drug concentration corresponding to a given response.\\  
The \texttt{getAUC()} function returns the area under the curve (AUC) estimated by the trapezoid rule and the Simpson's rule.\\  
The \texttt{getEstimates()} invert the function and returns the estimated concentration for a given response.  If no target is specified, the default output is a table of the x values corresponding to responses from 0.9 to 0.1.
<<>>=
getAUC(np1)
getEstimates(np1)
@

A single value (a target), or a vector of values, can be passed to \texttt{getEstimates()}, and a confidence level can be specified (by default, conf.level is set to .95).
<<>>=
getEstimates(np1, .5)
getEstimates(np1, c(.25, .5, .75), conf.level=.90)
@

\subsection{Example 2}
The next example analyses a drug-response experiment with replicated drug concentrations: the MCF-7 cell line treated with Irinotecan.
<<>>=
path <- system.file("extdata", "mcf7.txt", package="nplr")
mcf7 <- read.table(path, header=TRUE)
np2 <- nplr(x=mcf7$CONC, y=mcf7$GIPROP)
plot(np2, main="MCF-7 cell line. Response to Irinotecan", cex.main=1.25)
@

\vspace{.5in}
As there are replicates, we can compare the effect of the different weighted methods with the default $residuals weights$, on the fitting. A 'no-weight' condition can be tested by setting the \texttt{LPweight} argument to 0: The vector of weights is then a vector of 1s.
<<message=FALSE, warning=FALSE, fig.width=12, fig.height=10, echo=c(1:6, 8:11)>>=
x <- mcf7$CONC
y <- mcf7$GIPROP
noweight <- nplr(x, y, LPweight=0, silent=TRUE)
sdw <- nplr(x, y, method="sdw", silent=TRUE)
gw <-  nplr(x, y, method="sdw", LPweight=1.5, silent=TRUE)

par(mfrow=c(2,2))
plot(np2, showTarget=.5, main="residuals weights")
plot(noweight, showTarget=.5, main="No weight")
plot(sdw, showTarget=.5, main="Stdev weights")
plot(noweight, showTarget=.5, main="general weights")
par(op)
@

\vspace{.5in}
Note that the curves do not seem to change dramatically. However, the different weights can have different performances.

\subsection{Example 3}
\subsubsection{Fitting a Progression/Time model}
This last example illustrates a Progression/Time experiment: these are simulated data.\\
<<>>=
path <- system.file("extdata", "prog.txt", package="nplr")
prog <- read.table(path, header=TRUE)
@

Here, the progression values are given in some unknown unit. Here, the x values are \texttt{Time} in hours, and we don't need to use a $Log_10$ transformation. Next, let us assume that we have access to a $T_0$ and a $control$ values. We can use \texttt{convertToProp()} in order to convert the data to proportions. 
<<>>=
x <- prog$time
yp <- convertToProp(prog$prog, 5, 102)
np3 <- nplr(x, yp, useLog=FALSE)
@

\vspace{.5in}
When progression is at stake, it may be interesting to get the  coordinates of the inflexion point, as it corresponds to the point where the slope (the progression) is maximal.
<<>>=
plot(np3, showInfl=TRUE, xlab="Time (hrs)", ylab="Progression (prop. of control)", main="Progression", cex.main=2)
getInflexion(np3)
@

\subsubsection{Evaluating the number of parameters}
When a 5-p logistic regression is used, and because of the asymetric parameter, the curve is no longer symetrical around its inflexion point. Here is an illustration of the impact of the number of parameters on the fitting.
<<npar>>=
plot(x, yp, main="The n-parameter effect", xlab="Time", ylab="Progression", cex.main=1.5)
l <- c()
for(i in 2:5){
  test <- nplr(x, yp, npars=i, useLog=FALSE)
  lines(getXcurve(test), getYcurve(test), lwd=2, lty=3, , col=i)
  points(getInflexion(test), pch=15, cex=1.5, col=i)
  gof <- getGoodness(test)
  l <- c(l, sprintf("%s-P: GOF=%s", i, round(gof, 4)))
}
legend("bottomright", legend=l, lwd=2, col=2:5, bty="n")
@

\emph{Note that even if it is the case here, the 5-P model may not be always the best choice.}

\section{Accessing R code}
The R code for $nplr$ is available on github: \url{https://github.com/fredcommo/nplr}


\printbibliography

\end{document}
